{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXS14MDqjAdS"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y-NMgMlIa-_"
      },
      "source": [
        "# Auto-sklearn\n",
        "Auto-sklearn is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\n",
        "\n",
        "For more information about the framework, please visit the documentation [here](https://automl.github.io/auto-sklearn/master/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ihaCc5KtFL"
      },
      "source": [
        "## What you will need to run the code\n",
        "\n",
        "In order to run this code, we are going to first install Auto-sklearn using pip. For more instructions on how to install Auto-sklearn, for example using conda, please check [this](https://automl.github.io/auto-sklearn/master/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.8.1\n",
        "!pip install Cython==0.29.35\n",
        "# restart kernel!"
      ],
      "metadata": {
        "id": "Tsa70kzEts-f",
        "outputId": "1bbcc750-3751-4621-c939-4cdb45baeab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.8.1\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<1.25.0,>=1.17.3 (from scipy==1.8.1)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.4.26 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\n",
            "jaxlib 0.4.26+cuda12.cudnn89 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4 scipy-1.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "164cf28942b949e9bb62a5328aabb81c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Cython==0.29.35\n",
            "  Downloading Cython-0.29.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Cython\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.10\n",
            "    Uninstalling Cython-3.0.10:\n",
            "      Successfully uninstalled Cython-3.0.10\n",
            "Successfully installed Cython-0.29.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==0.24.2 --no-build-isolation"
      ],
      "metadata": {
        "id": "JFyjyyRPgqnc",
        "outputId": "76f74e67-288f-4d9a-e539-26cabf40acb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24.2\n",
            "  Using cached scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.24.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (3.5.0)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn: filename=scikit_learn-0.24.2-cp310-cp310-linux_x86_64.whl size=22196049 sha256=c9b03de661f00a12e88e471170117a62ca79e6d599c37f80eac032f0416890f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/a4/68/4e78865652fa14db4a162b491e5138565f97646f9e1f2ab8cc\n",
            "Successfully built scikit-learn\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.4.0 requires scikit-learn>=1.2.2, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "mlxtend 0.22.0 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-0.24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWhGa_OANAMM",
        "outputId": "6248c470-e4ec-4d1d-e4a2-5396810ef1bc"
      },
      "source": [
        "#this needs to run twice!\n",
        "!pip install auto-sklearn\n",
        "import autosklearn\n",
        "\n",
        "print(\"Using Auto-sklearn version {}\".format(autosklearn.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (4.11.0)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from auto-sklearn) (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.4.0)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: dask>=2021.12 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (2023.8.1)\n",
            "Requirement already satisfied: distributed>=2012.12 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (2023.8.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (6.0.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (2.0.3)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (3.5.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.21 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (0.4.21)\n",
            "Requirement already satisfied: pynisher<0.7,>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (0.6.4)\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (0.8.3)\n",
            "Requirement already satisfied: smac<1.3,>=1.2 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (0.29.35)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (3.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (1.4.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.12->auto-sklearn) (7.1.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (3.1.3)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2012.12->auto-sklearn) (3.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->auto-sklearn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->auto-sklearn) (2024.1)\n",
            "Requirement already satisfied: emcee>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from smac<1.3,>=1.2->auto-sklearn) (3.1.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2021.12->auto-sklearn) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2012.12->auto-sklearn) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->auto-sklearn) (1.16.0)\n",
            "Using Auto-sklearn version 0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUxbg5B2J8lU"
      },
      "source": [
        "## First Step: Load data\n",
        "\n",
        "Auto-sklearn can work with multiple input data formats (python lists, numpy arrays, sparse arrays and pandas data-frames).  \n",
        "\n",
        "For this example we are going to be using the [credit-g dataset](https://www.openml.org/d/31) which is a binary classification problem. This means that we have to find an estimator that is able to predict between 2 categories, *'bad'* and *'good'*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxGRtUSmJ7ki",
        "outputId": "670c3a4d-cd1e-44c7-f1bf-4b89ee637672"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "\n",
        "# We fetch the data using the openml.org\n",
        "X, y = sklearn.datasets.fetch_openml(data_id=31, return_X_y=True, as_frame=True)\n",
        "\n",
        "# Split the data into train and test\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42\n",
        ")\n",
        "\n",
        "X_train.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 600 entries, 24 to 102\n",
            "Data columns (total 20 columns):\n",
            " #   Column                  Non-Null Count  Dtype   \n",
            "---  ------                  --------------  -----   \n",
            " 0   checking_status         600 non-null    category\n",
            " 1   duration                600 non-null    float64 \n",
            " 2   credit_history          600 non-null    category\n",
            " 3   purpose                 600 non-null    category\n",
            " 4   credit_amount           600 non-null    float64 \n",
            " 5   savings_status          600 non-null    category\n",
            " 6   employment              600 non-null    category\n",
            " 7   installment_commitment  600 non-null    float64 \n",
            " 8   personal_status         600 non-null    category\n",
            " 9   other_parties           600 non-null    category\n",
            " 10  residence_since         600 non-null    float64 \n",
            " 11  property_magnitude      600 non-null    category\n",
            " 12  age                     600 non-null    float64 \n",
            " 13  other_payment_plans     600 non-null    category\n",
            " 14  housing                 600 non-null    category\n",
            " 15  existing_credits        600 non-null    float64 \n",
            " 16  job                     600 non-null    category\n",
            " 17  num_dependents          600 non-null    float64 \n",
            " 18  own_telephone           600 non-null    category\n",
            " 19  foreign_worker          600 non-null    category\n",
            "dtypes: category(13), float64(7)\n",
            "memory usage: 47.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovb0DEDfTRz1"
      },
      "source": [
        "## Second Step: Manually build a pipeline\n",
        "\n",
        "For this tutorial, we are going to implement some traditional machine learning models ([GradientBoosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html), [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html)) using [scikit-learn](https://scikit-learn.org/stable/index.html). Then we are going to show how we can achieve an even better performance than these traditional models, by using Auto-Sklearn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx8gyRP0cL_N"
      },
      "source": [
        "### C-Support Vector Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkhGLtCCT0i6",
        "outputId": "f2d8dff0-3282-49ea-f4cc-57e6fd46274e"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Create the estimator using the default parameters from the library\n",
        "estimator_svc = SVC(\n",
        "    C=1.0, kernel='rbf', gamma='scale', shrinking=True, tol=1e-3,\n",
        "    cache_size=200, verbose=False, max_iter=-1, random_state=42\n",
        ")\n",
        "\n",
        "# build and fit the pipeline\n",
        "categorical_columns = [col for col in X_train.columns\n",
        "                       if X[col].dtype.name == 'category']\n",
        "encoder = ColumnTransformer(transformers = [\n",
        "  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
        "], remainder='passthrough')\n",
        "pipeline_svc = Pipeline([\n",
        "  ('encoder', encoder),\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svc', estimator_svc),\n",
        "])\n",
        "pipeline_svc.fit(X_train, y_train)\n",
        "\n",
        "# Score the model\n",
        "prediction = pipeline_svc.predict(X_test)\n",
        "performance_svc = accuracy_score(y_test, prediction)\n",
        "print(f\"SVC performance is {performance_svc}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC performance is 0.7675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzcQy2lFnUVY"
      },
      "source": [
        "### GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmWaffHyTRCp",
        "outputId": "60914b6e-4914-4253-c97e-597d46071399"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Create the estimator using default parameters from the library\n",
        "estimator_gradboost = GradientBoostingClassifier(\n",
        "    learning_rate=0.1, n_estimators=100, subsample=1.0,\n",
        "    criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
        "    min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
        "    random_state=42)\n",
        "\n",
        "# Translate the categorical columns to\n",
        "# a numerical value\n",
        "categorical_columns = [col for col in X_train.columns\n",
        "                       if X[col].dtype.name == 'category']\n",
        "encoder = ColumnTransformer(transformers = [\n",
        "  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "\n",
        "# Build and fit the pipeline\n",
        "pipeline_gradboost = Pipeline([\n",
        "  ('encoder', encoder),\n",
        "  ('gradboost', estimator_gradboost),\n",
        "])\n",
        "pipeline_gradboost.fit(X_train, y_train)\n",
        "\n",
        "# Score the model\n",
        "prediction = pipeline_gradboost.predict(X_test)\n",
        "performance_gradboost = accuracy_score(y_test, prediction)\n",
        "print(f\"GradientBooster performance is {performance_gradboost}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBooster performance is 0.735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur1jyA7Ncc8M"
      },
      "source": [
        "### Decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE_s9S3Pef0E",
        "outputId": "6596fcbd-175b-467d-9607-aa7fb7888c7c"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create the estimator using the default parameters from the library\n",
        "estimator_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# build and fit the pipeline\n",
        "encoder = ColumnTransformer(transformers = [\n",
        "  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
        "], remainder='passthrough')\n",
        "pipeline_tree = Pipeline([\n",
        "  ('encoder', encoder),\n",
        "  ('DecisionTree', estimator_tree),\n",
        "])\n",
        "pipeline_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the training data\n",
        "prediction = pipeline_tree.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "performance_tree = accuracy_score(y_test, prediction)\n",
        "print(f\"Decision Tree performance is {performance_tree}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree performance is 0.7075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjIrVVFCf7PP"
      },
      "source": [
        "# Third Step: Use Auto-sklearn as a drop-in-replacement\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6a9W6xef7bI",
        "outputId": "16eafafc-d653-4eda-aaa4-1c98de77ffe0"
      },
      "source": [
        "import autosklearn.classification\n",
        "\n",
        "# Create and train the estimator\n",
        "estimator_askl = autosklearn.classification.AutoSklearnClassifier(\n",
        "    time_left_for_this_task=300,\n",
        "    seed=42,\n",
        "    resampling_strategy='cv',\n",
        "    n_jobs=1,\n",
        ")\n",
        "estimator_askl.fit(X_train, y_train)\n",
        "\n",
        "# Score the model\n",
        "prediction = estimator_askl.predict(X_test)\n",
        "performance_askl = accuracy_score(y_test, prediction)\n",
        "print(f\"Auto-Sklearn Classifier performance is {performance_askl}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autosklearn/data/target_validator.py:187: UserWarning: Fitting transformer with a pandas series which has the dtype category. Inverse transform may not be able preserve dtype when converting to np.ndarray\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQojc2E6jIWA"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "\n",
        "# By using Auto-Sklearn on can achieve a better performance!\n",
        "data_table.DataTable(\n",
        "pd.DataFrame(\n",
        "    [\n",
        "     {'Model': 'Auto-Sklearn Classifier', 'Accuracy': performance_askl},\n",
        "     {'Model': 'GradientBoosting', 'Accuracy': performance_gradboost},\n",
        "     {'Model': 'Decision Tree Classifier', 'Accuracy': performance_tree},\n",
        "     {'Model': 'Support Vector Classifier', 'Accuracy': performance_svc},\n",
        "     ]\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise:\n",
        "* Setup MLFlow (see [Week 3](https://colab.research.google.com/github/keuperj/MLSystems24/blob/main/week_3/MLFlow_Tutorial.ipynb) ) and tack the different hyper-parameter runs in the example above\n",
        "* generate plots to find the best model and hyperparameters"
      ],
      "metadata": {
        "id": "BJtK9BxI-hIV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PbZd522q-_to"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}